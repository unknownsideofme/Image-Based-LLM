{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "from dotenv import load_dotenv\n",
    "from langchain_ollama.embeddings import OllamaEmbeddings\n",
    "from langchain_groq import ChatGroq\n",
    "from langchain_core.prompts import ChatPromptTemplate"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "load_dotenv()\n",
    "api_key = os.getenv(\"GROQ_API_KEY\")\n",
    "if api_key is None:\n",
    "    raise ValueError(\"API Key not set. Please set the OPENAI_API_KEY environment variable.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "llm = ChatGroq(api_key=api_key, model=\"llama-3.2-90b-vision-preview\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "d:\\CodeBook\\CropDis\\.venv\\Lib\\site-packages\\tqdm\\auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "from langchain.tools import BaseTool\n",
    "from transformers import BlipProcessor, BlipForConditionalGeneration, DetrImageProcessor, DetrForObjectDetection\n",
    "from PIL import Image\n",
    "import torch\n",
    "\n",
    "import os\n",
    "from tempfile import NamedTemporaryFile\n",
    "from langchain.agents import initialize_agent\n",
    "from langchain.chat_models import ChatOpenAI\n",
    "from langchain.chains.conversation.memory import ConversationBufferWindowMemory"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "from PIL import Image\n",
    "from transformers import BlipProcessor, BlipForConditionalGeneration, DetrImageProcessor, DetrForObjectDetection\n",
    "from langchain.tools import BaseTool\n",
    "import torch\n",
    "\n",
    "class ImageCaptionTool(BaseTool):\n",
    "    name: str = \"Image captioner\"  # Explicit type annotation\n",
    "    description: str = (\n",
    "        \"Use this tool when given the path to an image that you would like to be described.\"\n",
    "    )\n",
    "\n",
    "    def _run(self, img_path: str) -> str:\n",
    "        image = Image.open(img_path).convert(\"RGB\")\n",
    "\n",
    "        model_name = \"Salesforce/blip-image-captioning-large\"\n",
    "        device = \"cpu\"  # Change to \"cuda\" if GPU is available\n",
    "\n",
    "        processor = BlipProcessor.from_pretrained(model_name)\n",
    "        model = BlipForConditionalGeneration.from_pretrained(model_name).to(device)\n",
    "\n",
    "        inputs = processor(image, return_tensors=\"pt\").to(device)\n",
    "        output = model.generate(**inputs, max_new_tokens=20)\n",
    "\n",
    "        caption = processor.decode(output[0], skip_special_tokens=True)\n",
    "\n",
    "        return caption\n",
    "\n",
    "    def _arun(self, query: str):\n",
    "        raise NotImplementedError(\"This tool does not support async\")\n",
    "\n",
    "class ObjectDetectionTool(BaseTool):\n",
    "    name: str = \"Object detector\"  # Explicit type annotation\n",
    "    description: str = (\n",
    "        \"Use this tool when given the path to an image that you would like to detect objects. \"\n",
    "        \"It will return a list of all detected objects. Each element in the list is in the format: \"\n",
    "        \"[x1, y1, x2, y2] class_name confidence_score.\"\n",
    "    )\n",
    "\n",
    "    def _run(self, img_path: str) -> str:\n",
    "        image = Image.open(img_path).convert(\"RGB\")\n",
    "\n",
    "        processor = DetrImageProcessor.from_pretrained(\"facebook/detr-resnet-50\")\n",
    "        model = DetrForObjectDetection.from_pretrained(\"facebook/detr-resnet-50\")\n",
    "\n",
    "        inputs = processor(images=image, return_tensors=\"pt\")\n",
    "        outputs = model(**inputs)\n",
    "\n",
    "        # Convert outputs (bounding boxes and class logits) to COCO API\n",
    "        target_sizes = torch.tensor([image.size[::-1]])\n",
    "        results = processor.post_process_object_detection(outputs, target_sizes=target_sizes, threshold=0.9)[0]\n",
    "\n",
    "        detections = \"\"\n",
    "        for score, label, box in zip(results[\"scores\"], results[\"labels\"], results[\"boxes\"]):\n",
    "            detections += \"[{}, {}, {}, {}]\".format(int(box[0]), int(box[1]), int(box[2]), int(box[3]))\n",
    "            detections += \" {}\".format(model.config.id2label[int(label)])\n",
    "            detections += \" {}\\n\".format(float(score))\n",
    "\n",
    "        return detections\n",
    "\n",
    "    def _arun(self, query: str):\n",
    "        raise NotImplementedError(\"This tool does not support async\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_image_caption(image_path):\n",
    "    \"\"\"\n",
    "    Generates a short caption for the provided image.\n",
    "\n",
    "    Args:\n",
    "        image_path (str): The path to the image file.\n",
    "\n",
    "    Returns:\n",
    "        str: A string representing the caption for the image.\n",
    "    \"\"\"\n",
    "    image = Image.open(image_path).convert('RGB')\n",
    "\n",
    "    model_name = \"Salesforce/blip-image-captioning-large\"\n",
    "    device = \"cpu\"  # cuda\n",
    "\n",
    "    processor = BlipProcessor.from_pretrained(model_name)\n",
    "    model = BlipForConditionalGeneration.from_pretrained(model_name).to(device)\n",
    "\n",
    "    inputs = processor(image, return_tensors='pt').to(device)\n",
    "    output = model.generate(**inputs, max_new_tokens=20)\n",
    "\n",
    "    caption = processor.decode(output[0], skip_special_tokens=True)\n",
    "\n",
    "    return caption\n",
    "\n",
    "\n",
    "def detect_objects(image_path):\n",
    "    \"\"\"\n",
    "    Detects objects in the provided image.\n",
    "\n",
    "    Args:\n",
    "        image_path (str): The path to the image file.\n",
    "\n",
    "    Returns:\n",
    "        str: A string with all the detected objects. Each object as '[x1, x2, y1, y2, class_name, confindence_score]'.\n",
    "    \"\"\"\n",
    "    image = Image.open(image_path).convert('RGB')\n",
    "\n",
    "    processor = DetrImageProcessor.from_pretrained(\"facebook/detr-resnet-50\")\n",
    "    model = DetrForObjectDetection.from_pretrained(\"facebook/detr-resnet-50\")\n",
    "\n",
    "    inputs = processor(images=image, return_tensors=\"pt\")\n",
    "    outputs = model(**inputs)\n",
    "\n",
    "    # convert outputs (bounding boxes and class logits) to COCO API\n",
    "    # let's only keep detections with score > 0.9\n",
    "    target_sizes = torch.tensor([image.size[::-1]])\n",
    "    results = processor.post_process_object_detection(outputs, target_sizes=target_sizes, threshold=0.9)[0]\n",
    "\n",
    "    detections = \"\"\n",
    "    for score, label, box in zip(results[\"scores\"], results[\"labels\"], results[\"boxes\"]):\n",
    "        detections += '[{}, {}, {}, {}]'.format(int(box[0]), int(box[1]), int(box[2]), int(box[3]))\n",
    "        detections += ' {}'.format(model.config.id2label[int(label)])\n",
    "        detections += ' {}\\n'.format(float(score))\n",
    "\n",
    "    return detections"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Debanjan\\AppData\\Local\\Temp\\ipykernel_27644\\3271607266.py:6: LangChainDeprecationWarning: Please see the migration guide at: https://python.langchain.com/docs/versions/migrating_memory/\n",
      "  conversational_memory = ConversationBufferWindowMemory(\n",
      "C:\\Users\\Debanjan\\AppData\\Local\\Temp\\ipykernel_27644\\3271607266.py:14: LangChainDeprecationWarning: LangChain agents will continue to be supported, but it is recommended for new use cases to be built with LangGraph. LangGraph offers a more flexible and full-featured framework for building agents, including support for tool-calling, persistence of state, and human-in-the-loop workflows. For details, refer to the `LangGraph documentation <https://langchain-ai.github.io/langgraph/>`_ as well as guides for `Migrating from AgentExecutor <https://python.langchain.com/docs/how_to/migrate_agent/>`_ and LangGraph's `Pre-built ReAct agent <https://langchain-ai.github.io/langgraph/how-tos/create-react-agent/>`_.\n",
      "  agent = initialize_agent(\n"
     ]
    }
   ],
   "source": [
    "from getpass import getpass\n",
    "msg = \"tell me what disease can this  be ?\"\n",
    "#initialize the gent\n",
    "tools = [ImageCaptionTool(), ObjectDetectionTool()]\n",
    "\n",
    "conversational_memory = ConversationBufferWindowMemory(\n",
    "    memory_key='chat_history',\n",
    "    k=5,\n",
    "    return_messages=True\n",
    ")\n",
    "\n",
    "\n",
    "\n",
    "agent = initialize_agent(\n",
    "    agent=\"chat-conversational-react-description\",\n",
    "    tools=tools,\n",
    "    llm=llm,\n",
    "    max_iterations=5,\n",
    "    verbose=True,\n",
    "    memory=conversational_memory,\n",
    "    early_stopping_method='generate'\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "\u001b[1m> Entering new AgentExecutor chain...\u001b[0m\n",
      "\u001b[32;1m\u001b[1;3m```json\n",
      "{\n",
      "    \"action\": \"Image captioner\",\n",
      "    \"action_input\": \"images (1).jpg\"\n",
      "}\n",
      "```\u001b[0m\n",
      "Observation: \u001b[36;1m\u001b[1;3mstudents raising their hands in a classroom with a teacher in the background\u001b[0m\n",
      "Thought:\u001b[32;1m\u001b[1;3m```json\n",
      "{\n",
      "    \"action\": \"Final Answer\",\n",
      "    \"action_input\": \"There is no indication of a specific disease in the provided image. It appears to be a normal classroom scene with students and a teacher. If you can provide more context or information, I'll do my best to help.\"\n",
      "}\n",
      "```\u001b[0m\n",
      "\n",
      "\u001b[1m> Finished chain.\u001b[0m\n",
      "{'input': 'tell me what disease can this  be ?, this is the image path: images (1).jpg', 'chat_history': [], 'output': \"There is no indication of a specific disease in the provided image. It appears to be a normal classroom scene with students and a teacher. If you can provide more context or information, I'll do my best to help.\"}\n"
     ]
    }
   ],
   "source": [
    "image_path = \"images (1).jpg\"\n",
    "user_question = msg\n",
    "response = agent.invoke(f'{user_question}, this is the image path: {image_path}')\n",
    "print(response)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".penv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
